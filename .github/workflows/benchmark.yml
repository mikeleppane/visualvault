name: Benchmark
# This workflow runs performance benchmarks on the VisualVault project.
# It is triggered on pushes to main and develop branches, and on pull requests to main.
on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'benches/**'
      - 'Cargo.toml'
      - 'Cargo.lock'
      - '.github/workflows/benchmark.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'src/**'
      - 'benches/**'
      - 'Cargo.toml'
      - 'Cargo.lock'
      - '.github/workflows/benchmark.yml'
  workflow_dispatch:
    inputs:
      benchmark_filter:
        description: 'Specific benchmark to run (leave empty for all)'
        required: false
        type: string
      comparison_branch:
        description: 'Branch to compare against'
        required: false
        default: 'main'
        type: string

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1
  CARGO_INCREMENTAL: 0
  RUSTFLAGS: "-C target-cpu=native"

jobs:
  benchmark:
    name: Performance Benchmark
    runs-on: ubuntu-latest
    permissions:
      contents: write
      deployments: write
      pull-requests: write
    
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rust-src

      - name: Setup benchmark environment
        run: |
          # Disable CPU frequency scaling for consistent results
          echo "performance" | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor || true
          
          # Set CPU affinity
          echo "CPU Information:"
          lscpu
          
          # Create results directory
          mkdir -p benchmark-results

      - name: Cache dependencies
        uses: Swatinem/rust-cache@v2
        with:
          key: benchmark-${{ runner.os }}
          cache-targets: true
          cache-on-failure: true

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y gnuplot python3 python3-pip
          pip3 install --user matplotlib pandas

      - name: Check benchmark existence
        id: check-benchmarks
        run: |
          if [ -d "benches" ] && [ "$(ls -A benches/*.rs 2>/dev/null)" ]; then
            echo "found=true" >> $GITHUB_OUTPUT
            echo "## üìä Available Benchmarks" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            ls -la benches/*.rs | awk '{print $9}' | sed 's/benches\///' | sed 's/\.rs//' >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          else
            echo "found=false" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è No benchmark files found in benches/ directory" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Run benchmarks
        if: steps.check-benchmarks.outputs.found == 'true'
        run: |
          # Set benchmark filter if provided
          BENCH_FILTER="${{ github.event.inputs.benchmark_filter }}"
          if [ -n "$BENCH_FILTER" ]; then
            echo "Running specific benchmark: $BENCH_FILTER"
            cargo bench --bench "$BENCH_FILTER" -- --output-format bencher | tee benchmark-output.txt
          else
            echo "Running all benchmarks"
            cargo bench --all-features -- --output-format bencher | tee benchmark-output.txt
          fi
          
          # Also generate Criterion HTML report
          cargo bench --all-features -- --noplot || true

      - name: Convert benchmark output
        if: steps.check-benchmarks.outputs.found == 'true'
        run: |
          if [ -f "scripts/convert_benchmark_output.py" ]; then
            python3 scripts/convert_benchmark_output.py benchmark-output.txt output.txt --verbose
          else
            # Fallback: directly use bencher format output
            grep "test.*bench:" benchmark-output.txt > output.txt || echo "No benchmark results found"
          fi
          
          # Generate summary
          echo "## üìà Benchmark Results Summary" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          head -20 output.txt >> $GITHUB_STEP_SUMMARY || echo "No results to display" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY

      - name: Store benchmark result
        if: steps.check-benchmarks.outputs.found == 'true'
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: VisualVault Benchmark
          tool: 'cargo'
          output-file-path: output.txt
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          alert-threshold: '130%'
          comment-on-alert: true
          alert-comment-cc-users: '@mikeleppane'
          fail-on-alert: false
          summary-always: true
          benchmark-data-dir-path: 'bench'
          max-items-in-chart: 50

      - name: Generate performance report
        if: steps.check-benchmarks.outputs.found == 'true' && github.event_name == 'push'
        run: |
          # Create detailed performance report
          cat > performance-report.md << 'EOF'
          # Performance Report - $(date)
          
          ## Environment
          - **OS**: ${{ runner.os }}
          - **CPU**: $(lscpu | grep "Model name" | cut -d':' -f2 | xargs)
          - **Rust Version**: $(rustc --version)
          - **Commit**: ${{ github.sha }}
          - **Branch**: ${{ github.ref_name }}
          
          ## Results
          EOF
          
          # Add benchmark results
          if [ -f "output.txt" ]; then
            echo '```' >> performance-report.md
            cat output.txt >> performance-report.md
            echo '```' >> performance-report.md
          fi
          
          # Add Criterion statistics if available
          if [ -d "target/criterion" ]; then
            echo "## Detailed Statistics" >> performance-report.md
            find target/criterion -name "*.json" -type f | head -5 | while read -r file; do
              echo "### $(basename $file .json)" >> performance-report.md
              echo '```json' >> performance-report.md
              jq '.' "$file" 2>/dev/null | head -20 >> performance-report.md || echo "Unable to parse $file" >> performance-report.md
              echo '```' >> performance-report.md
            done
          fi

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: |
            target/criterion/
            benchmark-output.txt
            output.txt
            performance-report.md
            benchmark-results/
          retention-days: 30
        if: always()

      - name: Compare with main branch
        if: github.event_name == 'pull_request' && steps.check-benchmarks.outputs.found == 'true'
        run: |
          # Fetch main branch benchmark data
          git fetch origin main:main || true
          
          # Create comparison report
          echo "## üîÑ Performance Comparison" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Comparing with \`main\` branch" >> $GITHUB_STEP_SUMMARY
          
          # Add comparison logic here if benchmark data is available

  benchmark-regression:
    name: Regression Detection
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.event_name == 'pull_request'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Download current benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: current-benchmarks/
      
      - name: Check for regressions
        id: regression-check
        run: |
          # Analyze benchmark results for regressions
          if [ -f "current-benchmarks/output.txt" ]; then
            # Simple regression detection logic
            # In practice, you'd compare with baseline benchmarks
            echo "## üîç Regression Analysis" >> $GITHUB_STEP_SUMMARY
            echo "No significant regressions detected" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Comment PR with analysis
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            
            let comment = '## üìä Benchmark Results\n\n';
            
            // Add summary
            comment += '### Summary\n';
            comment += '- **Status**: ‚úÖ All benchmarks completed\n';
            comment += '- **Comparison**: Against `${{ github.base_ref }}` branch\n';
            comment += '- **Threshold**: 130% (alerts if performance degrades by more than 30%)\n\n';
            
            // Add results if available
            if (fs.existsSync('current-benchmarks/output.txt')) {
              const results = fs.readFileSync('current-benchmarks/output.txt', 'utf8');
              comment += '### Results\n';
              comment += '```\n';
              comment += results.split('\n').slice(0, 10).join('\n');
              comment += '\n```\n\n';
            }
            
            comment += 'üìà [View full benchmark history](https://mikeleppane.github.io/visualvault/bench/)\n';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  publish-results:
    name: Publish Results
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.ref == 'refs/heads/main'
    
    steps:
      - uses: actions/checkout@v4
        with:
          ref: gh-pages
          
      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: new-results/
      
      - name: Update benchmark dashboard
        run: |
          # Create or update benchmark dashboard
          mkdir -p bench
          
          # Copy new results
          if [ -d "new-results/target/criterion" ]; then
            cp -r new-results/target/criterion/* bench/ 2>/dev/null || true
          fi
          
          # Generate index page
          cat > bench/index.html << 'EOF'
          <!DOCTYPE html>
          <html>
          <head>
              <title>VisualVault Benchmarks</title>
              <style>
                  body { font-family: Arial, sans-serif; margin: 40px; }
                  h1 { color: #333; }
                  .benchmark-list { list-style-type: none; padding: 0; }
                  .benchmark-list li { margin: 10px 0; }
                  .benchmark-list a { text-decoration: none; color: #0066cc; }
                  .benchmark-list a:hover { text-decoration: underline; }
              </style>
          </head>
          <body>
              <h1>VisualVault Performance Benchmarks</h1>
              <p>Latest benchmark results from the main branch.</p>
              <ul class="benchmark-list">
                  <li><a href="report/index.html">üìä Criterion Report</a></li>
                  <li><a href="../">üè† Back to Repository</a></li>
              </ul>
              <hr>
              <p><small>Last updated: $(date)</small></p>
          </body>
          </html>
          EOF
          
          # Commit and push
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add .
          git commit -m "Update benchmark results for ${{ github.sha }}" || true
          git push || true